Pre-requests:
    1)Fill in the value of the required environment variables in "env.sh.template", rename it to "env.sh".
        Note:
        a. You will need 2 key-pairs:
            (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY),
            (EC2_KEY_PAIR_NAME, EC2_PRIVATE_KEY_PATH)
        b. Please remember to save the .pem file when you generate the ec2 key-pair for this specific region.
        c. In case you encounter an Permission 0644 error, please give your ec2-key-pair file(.pem file) read-only permission. (use: chmod 400 YOUR_KEY_FILE.pem)
        d. Create an EC2 security group that has an inbound rule with Type: "All Traffic", Protocol: "All", Source: "Anywhere" on AWS web console;
        e. Please specify a S3 bucket name where the output files and logs will be saved.

    2)Fill the aws_access_key_id and aws_secret_access_key in "credentials_template", and rename it "credentials", move the "credentials" to directory "~/.aws".

    3)Install the "json" package which is used to parse the AWS response. Please follow the instruction: https://github.com/trentm/json.
	4) Make sure Gradle is installed. 
==============================================================================================================
How to run:
    - Navigate to root file of this assignment.
    - Source the env.sh file;
        - Command: . env.sh
    - Package the assignment into Jar file.
        - Command: ./compile.sh;
    - Configure the job in "config/hidoop.conf"
        - First line: "LOCAL" or "EC2"
        - Second line: MASTER_PORT
        - Third line: SLAVE_PORT (need to be different from MASTER_PORT)
    - Create the cluster and upload Jar file and node address file. 
        - Command: ./start-cluster.sh NUM_OF_NODES
    - Run map-reduce job with Hidoop
		- Command: ./hidoop_run JOB_JAR_FILE_PATH INPUT_PATH OUTPUT_PATH
		- Example: ./hidoop_run wordcount/Job.jar s3://juncai001/input_wc s3://juncai001/output_wc
		- Wait for job to be completed
		- The log of master node will be in 'logs' directory
    - Stop the cluster
        - Command: ./stop-cluster.sh
==============================================================================================================
How to compile other Hadoop code with Hidoop:
	- Compile Hidoop.jar with './compile.sh'
	- In all import statements, replace 'hadoop' with 'hidoop'
	- Add 'Hidoop/Hidoop.jar' into dependencies.
	- Compile the code in a way you like.
==============================================================================================================
References:
    http://aws.amazon.com/documentation/
    http://stackoverflow.com/questions/8193768/trying-to-ssh-into-an-amazon-ec2-instance-permission-error
    http://docs.aws.amazon.com/cli/latest/userguide/cli-ec2-sg.html#configuring-a-security-group
    https://github.com/trentm/json
    https://github.com/apache/hadoop
