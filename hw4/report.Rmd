---
title: "Flight Ticket Price Analysis with Linear Regression"
author: "Jun Cai, Vikas Boddu"
date: February 11, 2016
output: pdf_document
---

This document describes an implementation of Mapreduce instance with Hadoop doing ticket price analysis on the OTP (On-Time Performance) dataset. The purpose of the application is to perform a simple linear regression on the ticket price. The features we used are distance and air time. The label is average ticket price. For each carrier whic is active in 2015, we finished two simple linear regression for it. One between distance and average ticket price. The other one between air time and average ticket price.

<h3>Implementation</h3>
<h4>Data Normalization</h4>
In order to get a good regression results, the features should be normalized. We normalized the data so that they have zero mean and one variance. A R script is used to go over all the data and find out the mean and standard deviation of the distance as well as the air time. Then these information is provided to Mapper along with the dataset. Mapper will normalize the features in map phrase.

<h4>Calculation of Normal Equation</h4>
We use normal equation to solve the linear regression problem. 

<h3>Regression Results</h3>

Following is the result graph created by R script. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
options(warn=-1)
suppressMessages(library(plyr))

# load stats
stats <- read.csv(file="tmp/stats", head=FALSE, row.names=NULL)
mean_d <- stats[1, 1]
mean_t <- stats[1, 2]
std_d <- stats[2, 1]
std_t <- stats[2, 2]
min_d <- stats[3, 1]
min_t <- stats[3, 2]
max_d <- stats[4, 1]
max_t <- stats[4, 2]

# load MR output
thetas <- data.frame(carrier=character(),
					 feature=character(),
					 t0=double(),
					 t1=double(),
					 stringsAsFactors=FALSE)
input_path <- "output"
filenames <- list.files(input_path, pattern="part-r-*", full.names=TRUE)
for (i in 1:length(filenames))
	thetas <- rbind(thetas, read.csv(file=filenames[i], head=FALSE, row.names=NULL))
names(thetas) <- c("carrier", "feature", "t1", "t0")
carriers <- unique(thetas$carrier)
n_carriers <- length(carriers)

# plot the fitting graph
# loop through top carriers and plot the mean prices for each month
plot_counter <- 0
opar <- par(no.readonly=TRUE)
par(mfrow=c(3, 2))
# get min and max price, then config the y-axis
colcolors <- rainbow(10)

x_d <- seq(min_d, max_d, length.out=5)
x_t <- seq(min_t, max_t, length.out=5)
for (i in 1:n_carriers)
{
	plot_counter <- plot_counter + 1
	ct0 <- thetas[thetas$carrier == carriers[i] & thetas$feature == "D", 3]
	ct1 <- thetas[thetas$carrier == carriers[i] & thetas$feature == "D", 4]
	y_d <- ct1 * ((x_d - mean_d) / std_d) + ct0

	plot(x_d, y_d, type="l",
		 main=carriers[i],
		 xlab="Distance/mile", ylab="Mean Ticket Price",
		 col="blue",
		 xlim=c(0, 6000),
		 ylim=c(0, 1200))
	text(x_d, y_d, round(y_d, 2))
	text(1000, 1100, paste(c("theta_0:", round(ct0, 2)), collapse=" "))
	text(1150, 950, paste(c("theta_1:", round(ct1, 2)), collapse=" "))

	ct0 <- thetas[thetas$carrier == carriers[i] & thetas$feature == "T", 3]
	ct1 <- thetas[thetas$carrier == carriers[i] & thetas$feature == "T", 4]
	y_t <- ct1 * ((x_t - mean_t) / std_t) + ct0
	plot(x_t, y_t, type="l",
		 main=carriers[i],
		 xlab="Air Time/minute", ylab="Mean Ticket Price",
		 col="green",
		 xlim=c(0, 800),
		 ylim=c(0, 1200))
	text(x_t, y_t, round(y_t, 2))
	text(100, 1100, paste(c("theta_0:", round(ct0, 2)), collapse=" "))
	text(110, 950, paste(c("theta_1:", round(ct1, 2)), collapse=" "))
}
par(opar)
```
Where the title of each plot is the carrier's name. The left column is for the regression using distance as the feature and the right column is for that using air time. Coefficients for each regression are also given in the plots as "theta_0" and "theta_1". "theta_0" is the coefficient of the constant, and "theta_1" is the coefficient of the feature. 

<h3>Conclusions</h3>
<h4>Comparing the Features</h4>
We think the distance is a better feature than air time when predicting the ticket price. From all the coefficients above, we can see that "theta_1" is always much larger then the "theta_0" which means the features always contribute much more to the result rather than the contstant does. The other observation is that, when both comparing to constant, distance is more important than air time. For example, the regression result for 'AA'
