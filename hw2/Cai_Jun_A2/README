Author:
	Jun Cai, cai.jun@husky.neu.edu

System:
	Linux

Prereuisites:
	For the Mapreduce:
		1. AWS CLI package (for configuring and running jobs on AWS)
		2. R
		3. Hadoop
		4. Java
		5. ssh (use key authentication so you can do 'ssh localhost' without a password prompt)
	For generating report:
		1. Kintr
		2. Pandoc: (apt-get install pandoc is not enough, need version 1.12.3 or up)
		3. Latex: sudo apt-get install texlive-full
		4. Fonts: sudo apt-get install texlive-fonts-recommended

Build and run the code:
	0. Have correct hadoop paths(both /bin and /sbin folder), setting in PATH environment variable.
	1. Add execute permission to run.sh by 'sudo chmod +x ./run.sh'
	2. Set JAVA_HOME in .hadoop/hadoop-env.sh and .hadoop/yarn-env.sh to the installation path of java.
	3. Set the environment variable MR_INPUT to the directory which contains all the gziped data file by 'export MR_INPUT=PATH_TO_DATA_DIR'
	4. Run the code in pseudo-distributed mode with './run.sh -pd'
	5. Run the code with AWS with './run.sh -emr'. Data files are already uploaded to a S3 bucket. If you want to re-upload data files, please use './run.sh -emr -EMRInput'. This will upload all the files in the MR_INPUT path to S3 bucket.(upload all the data files may take a long time so it may be a better idea to just used the existing data which is from A1)
	6. Report file will be created automatically during the process. The report file name is 'report.html'. Also there is a sample report generated by cluster mode named 'generated_report.html' for reference. 
	
